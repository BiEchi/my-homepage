---
# Activate this widget? true/false
active: true

# This file represents a page section.
headless: true

# Order that this section appears on the page.
weight: 30

title: My Views
---

**Interpretability**

In a discussion with Prof. [Raymond Yeh](https://scholar.google.com/citations?hl=zh-CN&user=7HDE1ZwAAAAJ), he mentioned that the importance of **interpretability** is dependent on **performance**. If we can already achieve good performance, interpretability is not a big deal. The importance of interpretability appears when something is **not good enough**. When this happens, we want to understand how the system works, in order to improve the system more efficienctly. 

If we don't go this way, interpretability usually decreases performance, because interpretability is intrinsically costly. Black-box models are called black-box because opening the box is **intrinsically unsupported**, thus trying to interpret black-box models always leads to extra effort, which potentially drags the performance down.

This is why **principled methods** are valuable: principled methods are **intrinsically interpretable**. When the method has a bad performance, it will not be published; when the method has a good performance, it's interpretable. Principled methods like [CRATE](https://github.com/Ma-Lab-Berkeley/CRATE) and [Information Band Theory](https://arxiv.org/pdf/1703.00810.pdf) are good examples of this claim. This is the reason why **I prefer principled methods compared to purely statistical methods.**

**Value of Research**

In a discussion with Prof. [Chengxiang Zhai](https://scholar.google.com/citations?hl=zh-CN&user=YU-baPIAAAAJ), he mentioned the key value of research across all subjects: a valuable research should either **propose a new question** or **propose a new solution to an existing question**. A new question is usually more difficult to define, because the value of the question is highly dependent on **both academic and real-world recognization**. The value of a  solution is evaluated by **metrics**, which are defined by the question, but the value of a question is evaluated by **human**, who is defined by physics.

Going either way, it's inevitable to do comprehensive survey before designing any methods. After proposing an idea, it's extremely important to do comprehensive survey on the topic to refine the idea and method. It's also important to give presentations on the idea to people from various backgrounds, including **home-area specialists** on the method feasibility, **away-area specialists** on approach value, and **out-of-area audience** on real-life value. For example, when you want to use ML-based methods to develop a new drug, you need home-area (ML) guys to look at your method, away-area (chemistry) guys to assess the value of the approach, and out-of-area audience (like your parents) to comment on high-level real-life value.

Under such framework, quick publication is not a good idea for a real researcher, because they lack a ton of considerations and real-life applications. As suggested by Prof. [Heng Ji](https://scholar.google.com/citations?hl=zh-CN&user=z7GCqT4AAAAJ), when she assesses a student, **quality is much more important than quantity**. A student must have 1st authorship paper published at top-tier conference, but as long as he has some, the number becomes insignificant. Having publishments means the student has participated in the complete process of a research, and having a good, solid and creative publishment means the student is good, strict and creative. On the other hand, having too many publications is extremely confusing to PIs when they view the CV and may lead to an impression that the student focuses more on **fame** per se instead of **research**. This kind of impression is extremely bad and usually causes an instinctive refusion.

**The Three Circles**





**Peer Pressure**





**Short-term and Long-term Research Goals**
